{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf4f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "import subprocess\n",
    "import shlex\n",
    "import json\n",
    "\n",
    "\n",
    "from krkn_lib.prometheus.krkn_prometheus import KrknPrometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ba2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMETHEUS_TOKEN=!oc whoami -t \n",
    "PROMETHEUS_TOKEN=PROMETHEUS_TOKEN[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b370009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://thanos-querier-openshift-monitoring.apps.rosa.eqxvi-f7vz7-2xx.vooh.p3.openshiftapps.com'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMETHEUS_HOST = !kubectl -n openshift-monitoring get route -l app.kubernetes.io/name=thanos-query -o json | jq --raw-output '.items[0].spec.host'\n",
    "PROMETHEUS_HOST = \"https://\" + PROMETHEUS_HOST[0]\n",
    "PROMETHEUS_HOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fa6093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-05-21T06:27:42.320565+00:00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now(datetime.timezone.utc).isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb184bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747808864.289058"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc672c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "krkn_prom = KrknPrometheus(\n",
    "    prometheus_url=PROMETHEUS_HOST,\n",
    "    prometheus_bearer_token=PROMETHEUS_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a9a263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krkn_prom.process_query('increase(kube_pod_container_status_restarts_total{namespace=\"robot-shop\"}[2m]) > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e92a836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metric': {}, 'values': [[1747808877, '0']]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krkn_prom.process_prom_query_in_range(\n",
    "    'sum(kube_pod_container_status_restarts_total{namespace=\"robot-shop\"})',\n",
    "    start_time=datetime.datetime.now(),\n",
    "    end_time=datetime.datetime.now(),\n",
    "    granularity=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e3c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d7201161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Config\n",
    "SCENARIOS = [\n",
    "    {\n",
    "        \"name\": \"application-outages\",\n",
    "        \"params\": {\n",
    "            \"namespace\": [ \"robot-shop\" ],\n",
    "            \"pod-selector\": [\"{service: cart}\", \"{service: catalogue}\", \"{service: dispatch}\",\"{service: mongodb}\",\"{service: mysql}\", \"{service: payment}\", \"{service: rabbitmq}\", \"{service: ratings}\", \"{service: redis}\", \"{service: shipping}\", \"{service: user}\", \"{service: web}\"],\n",
    "            \"block-traffic-type\": [\"[Ingress]\", \"[Egress]\"]\n",
    "        }\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"pod-scenarios\",\n",
    "    #     \"params\": {\n",
    "    #         \"namespace\": [\"openshift-dns\"],\n",
    "    #         \"name-pattern\": [\"dns-default.*\", \"node-resolver.*\"],\n",
    "    #     }\n",
    "    # }\n",
    "]\n",
    "\n",
    "KUBECONFIG = \"../tmp/kubeconfig.yaml\"\n",
    "\n",
    "PROMQL_METRIC = \"\"\n",
    "\n",
    "CHAOS_DURATION = 120\n",
    "\n",
    "POPULATION_SIZE = 10\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aef9a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_population(population_size=10):\n",
    "    population = []\n",
    "    \n",
    "    while len(population) != population_size:\n",
    "        random_scenario = random.choice(SCENARIOS)\n",
    "        population_member = { \"name\": random_scenario['name'], 'params': {} }\n",
    "        for param_name in random_scenario['params']:\n",
    "            population_member['params'][param_name] = random.choice(random_scenario['params'][param_name])\n",
    "        population.append(population_member)\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "37203487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'pod-scenarios',\n",
       " 'params': {'namespace': 'openshift-dns', 'name-pattern': 'dns-default.*'}}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_population = create_population(10)\n",
    "sample_population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a41dec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shell(command):\n",
    "    logs = \"\"\n",
    "    command = shlex.split(command)\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end=\"\")\n",
    "        logs += line\n",
    "    \n",
    "    process.wait()\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a91bcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "-rw-r--r--. 1 rashetty rashetty 13547 May 20 12:57 gen_algorithm.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'total 16\\n-rw-r--r--. 1 rashetty rashetty 13547 May 20 12:57 gen_algorithm.ipynb\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_shell(\"ls -l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b17d9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "PODMAN_TEMPLATE = 'podman run --env-host=true -e TELEMETRY_PROMETHEUS_BACKUP=\"False\" -e WAIT_DURATION=0 -e DURATION={chaos_duration} {env_list} --net=host -v {kubeconfig}:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:{name}'\n",
    "\n",
    "def run_krkn_scenario_with_hub(scenario, kubeconfig):\n",
    "    env_list = \"\"\n",
    "    \n",
    "    for param in scenario['params']:\n",
    "        param_value = scenario['params'][param]\n",
    "        # Krkn Hub param are uppercase env name with underscore\n",
    "        param_name = param.upper().replace('-', '_')\n",
    "        env = f' -e {param_name}=\"{param_value}\" '\n",
    "        env_list += env\n",
    "    \n",
    "    krkn_hub_command = PODMAN_TEMPLATE.format(\n",
    "        kubeconfig=kubeconfig,\n",
    "        name=scenario['name'],\n",
    "        env_list=env_list,\n",
    "        chaos_duration=CHAOS_DURATION\n",
    "    )\n",
    "    \n",
    "    print(\"Running command: \", krkn_hub_command)\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    log_out = run_shell(krkn_hub_command)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    \n",
    "    return {\n",
    "        \"cmd\": krkn_hub_command,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"log\": log_out,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9c5578f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'pod-scenarios',\n",
       " 'params': {'namespace': 'openshift-dns', 'name-pattern': 'dns-default.*'}}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "00a67181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command:  podman run --env-host=true -e WAIT_DURATION=0 -e DURATION=120  -e NAMESPACE=\"openshift-dns\"  -e NAME_PATTERN=\"dns-default.*\"  --net=host -v ../tmp/kubeconfig.yaml:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n",
      "\u001b[1m20-05-2025T14:19:14 Checking if OpenShift client is installed\u001b[0m\n",
      "/usr/bin/oc\n",
      " _              _              \n",
      "| | ___ __ __ _| | _____ _ __  \n",
      "| |/ / '__/ _` | |/ / _ \\ '_ \\ \n",
      "|   <| | | (_| |   <  __/ | | |\n",
      "|_|\\_\\_|  \\__,_|_|\\_\\___|_| |_|\n",
      "                               \n",
      "\n",
      "2025-05-20 14:19:15,423 [INFO] Starting kraken\n",
      "2025-05-20 14:19:15,429 [INFO] Initializing client to talk to the Kubernetes cluster\n",
      "2025-05-20 14:19:15,429 [INFO] Generated a uuid for the run: 4f3ca26a-bc36-4121-bdf4-002021432839\n",
      "2025-05-20 14:19:16,279 [INFO] Detected distribution openshift\n",
      "2025-05-20 14:19:22,840 [INFO] Fetching cluster info\n",
      "2025-05-20 14:19:25,327 [INFO] 4.18.13\n",
      "2025-05-20 14:19:25,327 [INFO] Server URL: https://api.ujoep-icmtj-7vx.c23q.p3.openshiftapps.com:443\n",
      "2025-05-20 14:19:25,327 [INFO] Daemon mode not enabled, will run through 1 iterations\n",
      "\n",
      "2025-05-20 14:19:26,540 [INFO] ðŸ“£ `ScenarioPluginFactory`: types from config.yaml mapped to respective classes for execution:\n",
      "2025-05-20 14:19:26,540 [INFO]   âœ… type: application_outages_scenarios âž¡ï¸ `ApplicationOutageScenarioPlugin` \n",
      "2025-05-20 14:19:26,540 [INFO]   âœ… type: container_scenarios âž¡ï¸ `ContainerScenarioPlugin` \n",
      "2025-05-20 14:19:26,540 [INFO]   âœ… type: hog_scenarios âž¡ï¸ `HogsScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: managedcluster_scenarios âž¡ï¸ `ManagedClusterScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… types: [pod_disruption_scenarios, pod_network_scenarios, ingress_node_scenarios] âž¡ï¸ `NativeScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: network_chaos_scenarios âž¡ï¸ `NetworkChaosScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: network_chaos_ng_scenarios âž¡ï¸ `NetworkChaosNgScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: node_scenarios âž¡ï¸ `NodeActionsScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: pvc_scenarios âž¡ï¸ `PvcScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: service_disruption_scenarios âž¡ï¸ `ServiceDisruptionScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: service_hijacking_scenarios âž¡ï¸ `ServiceHijackingScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: cluster_shut_down_scenarios âž¡ï¸ `ShutDownScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: syn_flood_scenarios âž¡ï¸ `SynFloodScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: time_scenarios âž¡ï¸ `TimeActionsScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO]   âœ… type: zone_outages_scenarios âž¡ï¸ `ZoneOutageScenarioPlugin` \n",
      "2025-05-20 14:19:26,541 [INFO] \n",
      "\n",
      "2025-05-20 14:19:26,542 [INFO] health checks config is not defined, skipping them\n",
      "2025-05-20 14:19:26,542 [INFO] Executing scenarios for iteration 0\n",
      "2025-05-20 14:19:26,544 [INFO] Running NativeScenarioPlugin: ['pod_disruption_scenarios', 'pod_network_scenarios', 'ingress_node_scenarios'] -> scenarios/pod_scenario.yaml\n",
      "2025-05-20 14:19:33,402 [INFO] waiting 120 seconds for pod recovery, pod name pattern: dns-default.* namespace pattern: openshift-dns\n",
      "2025-05-20 14:19:38,712 [INFO] {\n",
      "\t\"output_id\": \"success\",\n",
      "\t\"output_data\": {\n",
      "\t\t\"pods\": {\n",
      "\t\t\t\"1747750777196358075\": {\n",
      "\t\t\t\t\"namespace\": \"openshift-dns\",\n",
      "\t\t\t\t\"name\": \"dns-default-st9tp\",\n",
      "\t\t\t\t\"creation_timestamp\": \"2025-05-20 12:45:57+00:00\"\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "2025-05-20 14:21:38,714 [ERROR] NativeScenarioPlugin unrecovered pods: Thread pool did not shutdown correctly,aborting.\n",
      "Exception: \n",
      "2025-05-20 14:21:42,921 [INFO] Find cluster events in file /tmp/events.json\n",
      "2025-05-20 14:21:42,921 [INFO] wating 0 before running the next scenario\n",
      "2025-05-20 14:21:42,921 [INFO] collecting OCP cluster metadata, this may take few minutes....\n",
      "2025-05-20 14:23:00,524 [INFO] failed to parse virtualmachines API\n",
      "2025-05-20 14:23:00,531 [INFO] Chaos data:\n",
      "{\n",
      "    \"telemetry\": {\n",
      "        \"scenarios\": [\n",
      "            {\n",
      "                \"start_timestamp\": 1747750766,\n",
      "                \"end_timestamp\": 1747750898,\n",
      "                \"scenario\": \"scenarios/pod_scenario.yaml\",\n",
      "                \"scenario_type\": \"pod_disruption_scenarios\",\n",
      "                \"exit_status\": 1,\n",
      "                \"parameters_base64\": \"\",\n",
      "                \"parameters\": [\n",
      "                    {\n",
      "                        \"config\": {\n",
      "                            \"kill\": 1,\n",
      "                            \"krkn_pod_recovery_time\": 120,\n",
      "                            \"name_pattern\": \"dns-default.*\",\n",
      "                            \"namespace_pattern\": \"openshift-dns\",\n",
      "                            \"timeout\": 180\n",
      "                        },\n",
      "                        \"id\": \"kill-pods\"\n",
      "                    }\n",
      "                ],\n",
      "                \"affected_pods\": {\n",
      "                    \"recovered\": [],\n",
      "                    \"unrecovered\": [],\n",
      "                    \"error\": \"Thread pool did not shutdown correctly,aborting.\\nException: \"\n",
      "                },\n",
      "                \"affected_nodes\": [],\n",
      "                \"cluster_events\": []\n",
      "            }\n",
      "        ],\n",
      "        \"node_summary_infos\": [\n",
      "            {\n",
      "                \"count\": 2,\n",
      "                \"architecture\": \"amd64\",\n",
      "                \"instance_type\": \"m5.xlarge\",\n",
      "                \"nodes_type\": \"worker\",\n",
      "                \"kernel_version\": \"5.14.0-427.68.1.el9_4.x86_64\",\n",
      "                \"kubelet_version\": \"v1.31.8\",\n",
      "                \"os_version\": \"Red Hat Enterprise Linux CoreOS 418.94.202505062142-0\"\n",
      "            }\n",
      "        ],\n",
      "        \"node_taints\": [],\n",
      "        \"kubernetes_objects_count\": {\n",
      "            \"ConfigMap\": 271,\n",
      "            \"Pod\": 90,\n",
      "            \"Deployment\": 35,\n",
      "            \"Route\": 9,\n",
      "            \"Build\": 1\n",
      "        },\n",
      "        \"network_plugins\": [\n",
      "            \"OVNKubernetes\"\n",
      "        ],\n",
      "        \"timestamp\": \"2025-05-20T14:19:25Z\",\n",
      "        \"health_checks\": null,\n",
      "        \"total_node_count\": 2,\n",
      "        \"cloud_infrastructure\": \"AWS\",\n",
      "        \"cloud_type\": \"rosa\",\n",
      "        \"cluster_version\": \"4.18.13\",\n",
      "        \"major_version\": \"4.18\",\n",
      "        \"run_uuid\": \"4f3ca26a-bc36-4121-bdf4-002021432839\",\n",
      "        \"job_status\": false\n",
      "    },\n",
      "    \"critical_alerts\": null\n",
      "}\n",
      "2025-05-20 14:23:00,531 [INFO] telemetry collection disabled, skipping.\n",
      "2025-05-20 14:23:00,532 [ERROR] Post scenarios are still failing at the end of all iterations\n"
     ]
    }
   ],
   "source": [
    "sample_run = run_krkn_scenario_with_hub(sample_population[0], \"../tmp/kubeconfig.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "80eb20f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cmd': 'podman run --env-host=true -e WAIT_DURATION=0 -e DURATION=120  -e NAMESPACE=\"openshift-dns\"  -e NAME_PATTERN=\"dns-default.*\"  --net=host -v ../tmp/kubeconfig.yaml:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios',\n",
       " 'start_time': datetime.datetime(2025, 5, 20, 19, 49, 14, 609442),\n",
       " 'end_time': datetime.datetime(2025, 5, 20, 19, 53, 0, 933759),\n",
       " 'log': '\\x1b[1m20-05-2025T14:19:14 Checking if OpenShift client is installed\\x1b[0m\\n/usr/bin/oc\\n _              _              \\n| | ___ __ __ _| | _____ _ __  \\n| |/ / \\'__/ _` | |/ / _ \\\\ \\'_ \\\\ \\n|   <| | | (_| |   <  __/ | | |\\n|_|\\\\_\\\\_|  \\\\__,_|_|\\\\_\\\\___|_| |_|\\n                               \\n\\n2025-05-20 14:19:15,423 [INFO] Starting kraken\\n2025-05-20 14:19:15,429 [INFO] Initializing client to talk to the Kubernetes cluster\\n2025-05-20 14:19:15,429 [INFO] Generated a uuid for the run: 4f3ca26a-bc36-4121-bdf4-002021432839\\n2025-05-20 14:19:16,279 [INFO] Detected distribution openshift\\n2025-05-20 14:19:22,840 [INFO] Fetching cluster info\\n2025-05-20 14:19:25,327 [INFO] 4.18.13\\n2025-05-20 14:19:25,327 [INFO] Server URL: https://api.ujoep-icmtj-7vx.c23q.p3.openshiftapps.com:443\\n2025-05-20 14:19:25,327 [INFO] Daemon mode not enabled, will run through 1 iterations\\n\\n2025-05-20 14:19:26,540 [INFO] ðŸ“£ `ScenarioPluginFactory`: types from config.yaml mapped to respective classes for execution:\\n2025-05-20 14:19:26,540 [INFO]   âœ… type: application_outages_scenarios âž¡ï¸ `ApplicationOutageScenarioPlugin` \\n2025-05-20 14:19:26,540 [INFO]   âœ… type: container_scenarios âž¡ï¸ `ContainerScenarioPlugin` \\n2025-05-20 14:19:26,540 [INFO]   âœ… type: hog_scenarios âž¡ï¸ `HogsScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: managedcluster_scenarios âž¡ï¸ `ManagedClusterScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… types: [pod_disruption_scenarios, pod_network_scenarios, ingress_node_scenarios] âž¡ï¸ `NativeScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: network_chaos_scenarios âž¡ï¸ `NetworkChaosScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: network_chaos_ng_scenarios âž¡ï¸ `NetworkChaosNgScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: node_scenarios âž¡ï¸ `NodeActionsScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: pvc_scenarios âž¡ï¸ `PvcScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: service_disruption_scenarios âž¡ï¸ `ServiceDisruptionScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: service_hijacking_scenarios âž¡ï¸ `ServiceHijackingScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: cluster_shut_down_scenarios âž¡ï¸ `ShutDownScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: syn_flood_scenarios âž¡ï¸ `SynFloodScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: time_scenarios âž¡ï¸ `TimeActionsScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO]   âœ… type: zone_outages_scenarios âž¡ï¸ `ZoneOutageScenarioPlugin` \\n2025-05-20 14:19:26,541 [INFO] \\n\\n2025-05-20 14:19:26,542 [INFO] health checks config is not defined, skipping them\\n2025-05-20 14:19:26,542 [INFO] Executing scenarios for iteration 0\\n2025-05-20 14:19:26,544 [INFO] Running NativeScenarioPlugin: [\\'pod_disruption_scenarios\\', \\'pod_network_scenarios\\', \\'ingress_node_scenarios\\'] -> scenarios/pod_scenario.yaml\\n2025-05-20 14:19:33,402 [INFO] waiting 120 seconds for pod recovery, pod name pattern: dns-default.* namespace pattern: openshift-dns\\n2025-05-20 14:19:38,712 [INFO] {\\n\\t\"output_id\": \"success\",\\n\\t\"output_data\": {\\n\\t\\t\"pods\": {\\n\\t\\t\\t\"1747750777196358075\": {\\n\\t\\t\\t\\t\"namespace\": \"openshift-dns\",\\n\\t\\t\\t\\t\"name\": \"dns-default-st9tp\",\\n\\t\\t\\t\\t\"creation_timestamp\": \"2025-05-20 12:45:57+00:00\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n\\n2025-05-20 14:21:38,714 [ERROR] NativeScenarioPlugin unrecovered pods: Thread pool did not shutdown correctly,aborting.\\nException: \\n2025-05-20 14:21:42,921 [INFO] Find cluster events in file /tmp/events.json\\n2025-05-20 14:21:42,921 [INFO] wating 0 before running the next scenario\\n2025-05-20 14:21:42,921 [INFO] collecting OCP cluster metadata, this may take few minutes....\\n2025-05-20 14:23:00,524 [INFO] failed to parse virtualmachines API\\n2025-05-20 14:23:00,531 [INFO] Chaos data:\\n{\\n    \"telemetry\": {\\n        \"scenarios\": [\\n            {\\n                \"start_timestamp\": 1747750766,\\n                \"end_timestamp\": 1747750898,\\n                \"scenario\": \"scenarios/pod_scenario.yaml\",\\n                \"scenario_type\": \"pod_disruption_scenarios\",\\n                \"exit_status\": 1,\\n                \"parameters_base64\": \"\",\\n                \"parameters\": [\\n                    {\\n                        \"config\": {\\n                            \"kill\": 1,\\n                            \"krkn_pod_recovery_time\": 120,\\n                            \"name_pattern\": \"dns-default.*\",\\n                            \"namespace_pattern\": \"openshift-dns\",\\n                            \"timeout\": 180\\n                        },\\n                        \"id\": \"kill-pods\"\\n                    }\\n                ],\\n                \"affected_pods\": {\\n                    \"recovered\": [],\\n                    \"unrecovered\": [],\\n                    \"error\": \"Thread pool did not shutdown correctly,aborting.\\\\nException: \"\\n                },\\n                \"affected_nodes\": [],\\n                \"cluster_events\": []\\n            }\\n        ],\\n        \"node_summary_infos\": [\\n            {\\n                \"count\": 2,\\n                \"architecture\": \"amd64\",\\n                \"instance_type\": \"m5.xlarge\",\\n                \"nodes_type\": \"worker\",\\n                \"kernel_version\": \"5.14.0-427.68.1.el9_4.x86_64\",\\n                \"kubelet_version\": \"v1.31.8\",\\n                \"os_version\": \"Red Hat Enterprise Linux CoreOS 418.94.202505062142-0\"\\n            }\\n        ],\\n        \"node_taints\": [],\\n        \"kubernetes_objects_count\": {\\n            \"ConfigMap\": 271,\\n            \"Pod\": 90,\\n            \"Deployment\": 35,\\n            \"Route\": 9,\\n            \"Build\": 1\\n        },\\n        \"network_plugins\": [\\n            \"OVNKubernetes\"\\n        ],\\n        \"timestamp\": \"2025-05-20T14:19:25Z\",\\n        \"health_checks\": null,\\n        \"total_node_count\": 2,\\n        \"cloud_infrastructure\": \"AWS\",\\n        \"cloud_type\": \"rosa\",\\n        \"cluster_version\": \"4.18.13\",\\n        \"major_version\": \"4.18\",\\n        \"run_uuid\": \"4f3ca26a-bc36-4121-bdf4-002021432839\",\\n        \"job_status\": false\\n    },\\n    \"critical_alerts\": null\\n}\\n2025-05-20 14:23:00,531 [INFO] telemetry collection disabled, skipping.\\n2025-05-20 14:23:00,532 [ERROR] Post scenarios are still failing at the end of all iterations\\n'}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "23254974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metric': {}, 'values': [[1747751654, '0']]}]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krkn_prom.process_prom_query_in_range(\n",
    "    'sum(kube_pod_container_status_restarts_total{namespace=\"robot-shop\"})',\n",
    "    start_time=datetime.datetime.now(),\n",
    "    end_time=datetime.datetime.now(),\n",
    "    granularity=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5eb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time_a = (datetime.datetime.now() - datetime.timedelta(minutes=30))\n",
    "# time_b = datetime.datetime.now()\n",
    "\n",
    "# (time_b - time_a).total_seconds() // 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b035da",
   "metadata": {},
   "outputs": [],
   "source": [
    "FITNESS_SCORE = \"\"\n",
    "\n",
    "def restart_count_fitness_in_robotshop(start, end):\n",
    "    try:\n",
    "        restart_count_at_beginning = krkn_prom.process_prom_query_in_range(\n",
    "            'sum(kube_pod_container_status_restarts_total{namespace=\"robot-shop\"})',\n",
    "            start_time=start,\n",
    "            end_time=start,\n",
    "            granularity=100\n",
    "        )[0]['values'][-1][1]\n",
    "\n",
    "        \n",
    "        restart_count_at_end = krkn_prom.process_prom_query_in_range(\n",
    "            'sum(kube_pod_container_status_restarts_total{namespace=\"robot-shop\"})',\n",
    "            start_time=end,\n",
    "            end_time=end,\n",
    "            granularity=100\n",
    "        )[0]['values'][-1][1]\n",
    "        \n",
    "        return int(restart_count_at_end) - int(restart_count_at_beginning)\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e);\n",
    "        raise Exception(\"Something happened...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3ec07515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pod-scenarios', 'node-resolver.*', 'openshift-dns')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scenario_unique_key(scenario):\n",
    "    # convert scenario into tuple of keys for unique matching at later point\n",
    "    params = []\n",
    "    for param in sorted(scenario['params']):\n",
    "        value = scenario['params'][param]\n",
    "        params.append(value)\n",
    "    return tuple([scenario['name']] + params)\n",
    "\n",
    "scenario_unique_key(sample_population[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a8d71771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fitness(scenario):\n",
    "    # run test\n",
    "    scenario_result = run_krkn_scenario_with_hub(scenario, KUBECONFIG)\n",
    "    \n",
    "    # Calculate fitness\n",
    "    start = scenario_result['start_time']\n",
    "    end = scenario_result['end_time']\n",
    "    fitness_score =  restart_count_fitness_in_robotshop(start, end)\n",
    "    \n",
    "    # Create result\n",
    "    return {\n",
    "        'id': scenario_unique_key(scenario),\n",
    "        'scenario': scenario,\n",
    "        'result': scenario_result,\n",
    "        'fitness_score': fitness_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8c5efc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command:  podman run --env-host=true -e WAIT_DURATION=0 -e DURATION=120  -e NAMESPACE=\"openshift-dns\"  -e NAME_PATTERN=\"node-resolver.*\"  --net=host -v ../tmp/kubeconfig.yaml:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios\n",
      "\u001b[1m20-05-2025T14:45:49 Checking if OpenShift client is installed\u001b[0m\n",
      "/usr/bin/oc\n",
      " _              _              \n",
      "| | ___ __ __ _| | _____ _ __  \n",
      "| |/ / '__/ _` | |/ / _ \\ '_ \\ \n",
      "|   <| | | (_| |   <  __/ | | |\n",
      "|_|\\_\\_|  \\__,_|_|\\_\\___|_| |_|\n",
      "                               \n",
      "\n",
      "2025-05-20 14:45:49,727 [INFO] Starting kraken\n",
      "2025-05-20 14:45:49,734 [INFO] Initializing client to talk to the Kubernetes cluster\n",
      "2025-05-20 14:45:49,734 [INFO] Generated a uuid for the run: 10d66c47-f48a-4cf2-8227-3ea6a6ebf42b\n",
      "2025-05-20 14:45:50,738 [INFO] Detected distribution openshift\n",
      "2025-05-20 14:45:54,888 [INFO] Fetching cluster info\n",
      "2025-05-20 14:45:57,389 [INFO] 4.18.13\n",
      "2025-05-20 14:45:57,389 [INFO] Server URL: https://api.ujoep-icmtj-7vx.c23q.p3.openshiftapps.com:443\n",
      "2025-05-20 14:45:57,389 [INFO] Daemon mode not enabled, will run through 1 iterations\n",
      "\n",
      "2025-05-20 14:45:58,691 [INFO] ðŸ“£ `ScenarioPluginFactory`: types from config.yaml mapped to respective classes for execution:\n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: application_outages_scenarios âž¡ï¸ `ApplicationOutageScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: container_scenarios âž¡ï¸ `ContainerScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: hog_scenarios âž¡ï¸ `HogsScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: managedcluster_scenarios âž¡ï¸ `ManagedClusterScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… types: [pod_disruption_scenarios, pod_network_scenarios, ingress_node_scenarios] âž¡ï¸ `NativeScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: network_chaos_scenarios âž¡ï¸ `NetworkChaosScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: network_chaos_ng_scenarios âž¡ï¸ `NetworkChaosNgScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: node_scenarios âž¡ï¸ `NodeActionsScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: pvc_scenarios âž¡ï¸ `PvcScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: service_disruption_scenarios âž¡ï¸ `ServiceDisruptionScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: service_hijacking_scenarios âž¡ï¸ `ServiceHijackingScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: cluster_shut_down_scenarios âž¡ï¸ `ShutDownScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: syn_flood_scenarios âž¡ï¸ `SynFloodScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: time_scenarios âž¡ï¸ `TimeActionsScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO]   âœ… type: zone_outages_scenarios âž¡ï¸ `ZoneOutageScenarioPlugin` \n",
      "2025-05-20 14:45:58,691 [INFO] \n",
      "\n",
      "2025-05-20 14:45:58,692 [INFO] health checks config is not defined, skipping them\n",
      "2025-05-20 14:45:58,692 [INFO] Executing scenarios for iteration 0\n",
      "2025-05-20 14:45:58,694 [INFO] Running NativeScenarioPlugin: ['pod_disruption_scenarios', 'pod_network_scenarios', 'ingress_node_scenarios'] -> scenarios/pod_scenario.yaml\n",
      "2025-05-20 14:46:04,884 [INFO] waiting 120 seconds for pod recovery, pod name pattern: node-resolver.* namespace pattern: openshift-dns\n",
      "2025-05-20 14:46:10,812 [INFO] {\n",
      "\t\"output_id\": \"success\",\n",
      "\t\"output_data\": {\n",
      "\t\t\"pods\": {\n",
      "\t\t\t\"1747752369206277489\": {\n",
      "\t\t\t\t\"namespace\": \"openshift-dns\",\n",
      "\t\t\t\t\"name\": \"node-resolver-zf2gd\",\n",
      "\t\t\t\t\"creation_timestamp\": \"2025-05-20 12:45:19+00:00\"\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "2025-05-20 14:48:10,815 [ERROR] NativeScenarioPlugin unrecovered pods: Thread pool did not shutdown correctly,aborting.\n",
      "Exception: \n",
      "2025-05-20 14:48:14,555 [INFO] Find cluster events in file /tmp/events.json\n",
      "2025-05-20 14:48:14,556 [INFO] wating 0 before running the next scenario\n",
      "2025-05-20 14:48:14,556 [INFO] collecting OCP cluster metadata, this may take few minutes....\n",
      "2025-05-20 14:49:29,616 [INFO] failed to parse virtualmachines API\n",
      "2025-05-20 14:49:29,626 [INFO] Chaos data:\n",
      "{\n",
      "    \"telemetry\": {\n",
      "        \"scenarios\": [\n",
      "            {\n",
      "                \"start_timestamp\": 1747752358,\n",
      "                \"end_timestamp\": 1747752490,\n",
      "                \"scenario\": \"scenarios/pod_scenario.yaml\",\n",
      "                \"scenario_type\": \"pod_disruption_scenarios\",\n",
      "                \"exit_status\": 1,\n",
      "                \"parameters_base64\": \"\",\n",
      "                \"parameters\": [\n",
      "                    {\n",
      "                        \"config\": {\n",
      "                            \"kill\": 1,\n",
      "                            \"krkn_pod_recovery_time\": 120,\n",
      "                            \"name_pattern\": \"node-resolver.*\",\n",
      "                            \"namespace_pattern\": \"openshift-dns\",\n",
      "                            \"timeout\": 180\n",
      "                        },\n",
      "                        \"id\": \"kill-pods\"\n",
      "                    }\n",
      "                ],\n",
      "                \"affected_pods\": {\n",
      "                    \"recovered\": [],\n",
      "                    \"unrecovered\": [],\n",
      "                    \"error\": \"Thread pool did not shutdown correctly,aborting.\\nException: \"\n",
      "                },\n",
      "                \"affected_nodes\": [],\n",
      "                \"cluster_events\": []\n",
      "            }\n",
      "        ],\n",
      "        \"node_summary_infos\": [\n",
      "            {\n",
      "                \"count\": 2,\n",
      "                \"architecture\": \"amd64\",\n",
      "                \"instance_type\": \"m5.xlarge\",\n",
      "                \"nodes_type\": \"worker\",\n",
      "                \"kernel_version\": \"5.14.0-427.68.1.el9_4.x86_64\",\n",
      "                \"kubelet_version\": \"v1.31.8\",\n",
      "                \"os_version\": \"Red Hat Enterprise Linux CoreOS 418.94.202505062142-0\"\n",
      "            }\n",
      "        ],\n",
      "        \"node_taints\": [],\n",
      "        \"kubernetes_objects_count\": {\n",
      "            \"ConfigMap\": 271,\n",
      "            \"Pod\": 90,\n",
      "            \"Deployment\": 35,\n",
      "            \"Route\": 9,\n",
      "            \"Build\": 1\n",
      "        },\n",
      "        \"network_plugins\": [\n",
      "            \"OVNKubernetes\"\n",
      "        ],\n",
      "        \"timestamp\": \"2025-05-20T14:45:57Z\",\n",
      "        \"health_checks\": null,\n",
      "        \"total_node_count\": 2,\n",
      "        \"cloud_infrastructure\": \"AWS\",\n",
      "        \"cloud_type\": \"rosa\",\n",
      "        \"cluster_version\": \"4.18.13\",\n",
      "        \"major_version\": \"4.18\",\n",
      "        \"run_uuid\": \"10d66c47-f48a-4cf2-8227-3ea6a6ebf42b\",\n",
      "        \"job_status\": false\n",
      "    },\n",
      "    \"critical_alerts\": null\n",
      "}\n",
      "2025-05-20 14:49:29,626 [INFO] telemetry collection disabled, skipping.\n",
      "2025-05-20 14:49:29,626 [ERROR] Post scenarios are still failing at the end of all iterations\n"
     ]
    }
   ],
   "source": [
    "fitness_result = calculate_fitness(sample_population[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "36614f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ('pod-scenarios', 'node-resolver.*', 'openshift-dns'),\n",
       " 'scenario': {'name': 'pod-scenarios',\n",
       "  'params': {'namespace': 'openshift-dns', 'name-pattern': 'node-resolver.*'}},\n",
       " 'result': {'cmd': 'podman run --env-host=true -e WAIT_DURATION=0 -e DURATION=120  -e NAMESPACE=\"openshift-dns\"  -e NAME_PATTERN=\"node-resolver.*\"  --net=host -v ../tmp/kubeconfig.yaml:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:pod-scenarios',\n",
       "  'start_time': datetime.datetime(2025, 5, 20, 20, 15, 48, 845789),\n",
       "  'end_time': datetime.datetime(2025, 5, 20, 20, 19, 30, 18431),\n",
       "  'log': '\\x1b[1m20-05-2025T14:45:49 Checking if OpenShift client is installed\\x1b[0m\\n/usr/bin/oc\\n _              _              \\n| | ___ __ __ _| | _____ _ __  \\n| |/ / \\'__/ _` | |/ / _ \\\\ \\'_ \\\\ \\n|   <| | | (_| |   <  __/ | | |\\n|_|\\\\_\\\\_|  \\\\__,_|_|\\\\_\\\\___|_| |_|\\n                               \\n\\n2025-05-20 14:45:49,727 [INFO] Starting kraken\\n2025-05-20 14:45:49,734 [INFO] Initializing client to talk to the Kubernetes cluster\\n2025-05-20 14:45:49,734 [INFO] Generated a uuid for the run: 10d66c47-f48a-4cf2-8227-3ea6a6ebf42b\\n2025-05-20 14:45:50,738 [INFO] Detected distribution openshift\\n2025-05-20 14:45:54,888 [INFO] Fetching cluster info\\n2025-05-20 14:45:57,389 [INFO] 4.18.13\\n2025-05-20 14:45:57,389 [INFO] Server URL: https://api.ujoep-icmtj-7vx.c23q.p3.openshiftapps.com:443\\n2025-05-20 14:45:57,389 [INFO] Daemon mode not enabled, will run through 1 iterations\\n\\n2025-05-20 14:45:58,691 [INFO] ðŸ“£ `ScenarioPluginFactory`: types from config.yaml mapped to respective classes for execution:\\n2025-05-20 14:45:58,691 [INFO]   âœ… type: application_outages_scenarios âž¡ï¸ `ApplicationOutageScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: container_scenarios âž¡ï¸ `ContainerScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: hog_scenarios âž¡ï¸ `HogsScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: managedcluster_scenarios âž¡ï¸ `ManagedClusterScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… types: [pod_disruption_scenarios, pod_network_scenarios, ingress_node_scenarios] âž¡ï¸ `NativeScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: network_chaos_scenarios âž¡ï¸ `NetworkChaosScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: network_chaos_ng_scenarios âž¡ï¸ `NetworkChaosNgScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: node_scenarios âž¡ï¸ `NodeActionsScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: pvc_scenarios âž¡ï¸ `PvcScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: service_disruption_scenarios âž¡ï¸ `ServiceDisruptionScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: service_hijacking_scenarios âž¡ï¸ `ServiceHijackingScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: cluster_shut_down_scenarios âž¡ï¸ `ShutDownScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: syn_flood_scenarios âž¡ï¸ `SynFloodScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: time_scenarios âž¡ï¸ `TimeActionsScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO]   âœ… type: zone_outages_scenarios âž¡ï¸ `ZoneOutageScenarioPlugin` \\n2025-05-20 14:45:58,691 [INFO] \\n\\n2025-05-20 14:45:58,692 [INFO] health checks config is not defined, skipping them\\n2025-05-20 14:45:58,692 [INFO] Executing scenarios for iteration 0\\n2025-05-20 14:45:58,694 [INFO] Running NativeScenarioPlugin: [\\'pod_disruption_scenarios\\', \\'pod_network_scenarios\\', \\'ingress_node_scenarios\\'] -> scenarios/pod_scenario.yaml\\n2025-05-20 14:46:04,884 [INFO] waiting 120 seconds for pod recovery, pod name pattern: node-resolver.* namespace pattern: openshift-dns\\n2025-05-20 14:46:10,812 [INFO] {\\n\\t\"output_id\": \"success\",\\n\\t\"output_data\": {\\n\\t\\t\"pods\": {\\n\\t\\t\\t\"1747752369206277489\": {\\n\\t\\t\\t\\t\"namespace\": \"openshift-dns\",\\n\\t\\t\\t\\t\"name\": \"node-resolver-zf2gd\",\\n\\t\\t\\t\\t\"creation_timestamp\": \"2025-05-20 12:45:19+00:00\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n\\n2025-05-20 14:48:10,815 [ERROR] NativeScenarioPlugin unrecovered pods: Thread pool did not shutdown correctly,aborting.\\nException: \\n2025-05-20 14:48:14,555 [INFO] Find cluster events in file /tmp/events.json\\n2025-05-20 14:48:14,556 [INFO] wating 0 before running the next scenario\\n2025-05-20 14:48:14,556 [INFO] collecting OCP cluster metadata, this may take few minutes....\\n2025-05-20 14:49:29,616 [INFO] failed to parse virtualmachines API\\n2025-05-20 14:49:29,626 [INFO] Chaos data:\\n{\\n    \"telemetry\": {\\n        \"scenarios\": [\\n            {\\n                \"start_timestamp\": 1747752358,\\n                \"end_timestamp\": 1747752490,\\n                \"scenario\": \"scenarios/pod_scenario.yaml\",\\n                \"scenario_type\": \"pod_disruption_scenarios\",\\n                \"exit_status\": 1,\\n                \"parameters_base64\": \"\",\\n                \"parameters\": [\\n                    {\\n                        \"config\": {\\n                            \"kill\": 1,\\n                            \"krkn_pod_recovery_time\": 120,\\n                            \"name_pattern\": \"node-resolver.*\",\\n                            \"namespace_pattern\": \"openshift-dns\",\\n                            \"timeout\": 180\\n                        },\\n                        \"id\": \"kill-pods\"\\n                    }\\n                ],\\n                \"affected_pods\": {\\n                    \"recovered\": [],\\n                    \"unrecovered\": [],\\n                    \"error\": \"Thread pool did not shutdown correctly,aborting.\\\\nException: \"\\n                },\\n                \"affected_nodes\": [],\\n                \"cluster_events\": []\\n            }\\n        ],\\n        \"node_summary_infos\": [\\n            {\\n                \"count\": 2,\\n                \"architecture\": \"amd64\",\\n                \"instance_type\": \"m5.xlarge\",\\n                \"nodes_type\": \"worker\",\\n                \"kernel_version\": \"5.14.0-427.68.1.el9_4.x86_64\",\\n                \"kubelet_version\": \"v1.31.8\",\\n                \"os_version\": \"Red Hat Enterprise Linux CoreOS 418.94.202505062142-0\"\\n            }\\n        ],\\n        \"node_taints\": [],\\n        \"kubernetes_objects_count\": {\\n            \"ConfigMap\": 271,\\n            \"Pod\": 90,\\n            \"Deployment\": 35,\\n            \"Route\": 9,\\n            \"Build\": 1\\n        },\\n        \"network_plugins\": [\\n            \"OVNKubernetes\"\\n        ],\\n        \"timestamp\": \"2025-05-20T14:45:57Z\",\\n        \"health_checks\": null,\\n        \"total_node_count\": 2,\\n        \"cloud_infrastructure\": \"AWS\",\\n        \"cloud_type\": \"rosa\",\\n        \"cluster_version\": \"4.18.13\",\\n        \"major_version\": \"4.18\",\\n        \"run_uuid\": \"10d66c47-f48a-4cf2-8227-3ea6a6ebf42b\",\\n        \"job_status\": false\\n    },\\n    \"critical_alerts\": null\\n}\\n2025-05-20 14:49:29,626 [INFO] telemetry collection disabled, skipping.\\n2025-05-20 14:49:29,626 [ERROR] Post scenarios are still failing at the end of all iterations\\n'},\n",
       " 'fitness_score': 0}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "53342768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_parents(fitness_scores):\n",
    "    \"\"\"\n",
    "    Selects two parents using Roulette Wheel Selection (proportionate selection).\n",
    "    Higher fitness means higher chance of being selected.\n",
    "    \"\"\"\n",
    "    total_fitness = sum([x['fitness_score'] for x in fitness_scores])\n",
    "\n",
    "    scenarios = [x['scenario'] for x in fitness_scores]\n",
    "\n",
    "    if total_fitness == 0:  # Handle case where all fitness scores are zero\n",
    "        return random.choice(scenarios), random.choice(scenarios)\n",
    "\n",
    "    # Normalize fitness scores to get probabilities\n",
    "    probabilities = [x['fitness_score'] / total_fitness for x in fitness_scores]\n",
    "\n",
    "    # Select parents based on probabilities\n",
    "    parent1 = random.choices(scenarios, weights=probabilities, k=1)[0]\n",
    "    parent2 = random.choices(scenarios, weights=probabilities, k=1)[0]\n",
    "    return parent1, parent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4d811717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'pod-scenarios',\n",
       " 'params': {'namespace': 'openshift-dns', 'name-pattern': 'dns-default.*'}}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d6e77fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(scenarioA, scenarioB):\n",
    "    common_params = set(scenarioA['params'].keys()) & set(scenarioB['params'].keys()) - set(['namespace'])\n",
    "    if len(common_params) == 0:\n",
    "        # no common parameter, currenty we return parents as is and hope for mutation\n",
    "        # adopt some different strategy\n",
    "        return scenarioA, scenarioB\n",
    "    else:\n",
    "        # if there are common params, lets switch values between them\n",
    "        for param in common_params:\n",
    "            if random.random() < 0.8:\n",
    "                # swap param values\n",
    "                valueA = scenarioA['params'][param]\n",
    "                valueB = scenarioB['params'][param]\n",
    "\n",
    "                scenarioA['params'][param] = valueB\n",
    "                scenarioB['params'][param] = valueA\n",
    "\n",
    "        return scenarioA, scenarioB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5db722fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'application-outages', 'params': {'namespace': 'robot-shop', 'pod-selector': '{service: shipping}', 'block-traffic-type': '[Ingress]'}} {'name': 'application-outages', 'params': {'namespace': 'openshift-dns', 'pod-selector': '{service: dispatch}', 'block-traffic-type': '[Egress]'}}\n",
      "{'name': 'application-outages', 'params': {'namespace': 'robot-shop', 'pod-selector': '{service: dispatch}', 'block-traffic-type': '[Egress]'}} {'name': 'application-outages', 'params': {'namespace': 'openshift-dns', 'pod-selector': '{service: shipping}', 'block-traffic-type': '[Ingress]'}}\n"
     ]
    }
   ],
   "source": [
    "sample1 = sample_population[8]\n",
    "sample2 = sample_population[6]\n",
    "\n",
    "print(sample1, sample2)\n",
    "\n",
    "sample1, sample2 = crossover(sample1, sample2)\n",
    "\n",
    "print(sample1, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9ffa9ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'application-outages',\n",
       "  'params': {'namespace': ['robot-shop'],\n",
       "   'pod-selector': ['{service: cart}',\n",
       "    '{service: catalogue}',\n",
       "    '{service: dispatch}',\n",
       "    '{service: mongodb}',\n",
       "    '{service: mysql}',\n",
       "    '{service: rabbitmq}',\n",
       "    '{service: ratings}',\n",
       "    '{service: shipping}',\n",
       "    '{service: user}',\n",
       "    '{service: web}'],\n",
       "   'block-traffic-type': []}}]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCENARIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b5a72039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up_value_for_scenario_param(name, value, param):\n",
    "    scenario_copy = SCENARIOS.copy()\n",
    "    for scenario in scenario_copy:\n",
    "        if scenario['name'] == name:\n",
    "            param_values = [x for x in scenario['params'][param]]\n",
    "            param_values.remove(value)\n",
    "            if len(param_values) == 0:\n",
    "                return value\n",
    "            return random.choice(param_values)\n",
    "    return value\n",
    "\n",
    "def mutate(scenario):\n",
    "    name = scenario['name']\n",
    "    for param in scenario['params'].keys():\n",
    "        if param != 'namespace' and random.random() < 0.3:\n",
    "            if isinstance(param, str):\n",
    "                value = scenario['params'][param]\n",
    "                scenario['params'][param] = look_up_value_for_scenario_param(name, value, param)\n",
    "    return scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4c9c6087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'application-outages', 'params': {'namespace': 'robot-shop', 'pod-selector': '{service: cart}', 'block-traffic-type': '[Ingress]'}}\n",
      "{'name': 'application-outages', 'params': {'namespace': 'robot-shop', 'pod-selector': '{service: catalogue}', 'block-traffic-type': '[Egress]'}}\n"
     ]
    }
   ],
   "source": [
    "sample1 = sample_population[8]\n",
    "\n",
    "print(sample1)\n",
    "\n",
    "sample1 = mutate(sample1)\n",
    "\n",
    "print(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "30a7e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algorithm(init_population_size=10, generations=10):\n",
    "    if init_population_size == 0 or generations == 0:\n",
    "        print(\"Please specify population and generations greater than 0.\")\n",
    "        return\n",
    "\n",
    "    population = create_population(init_population_size)\n",
    "\n",
    "    best_fitness = []\n",
    "    population_processed_so_far = {}\n",
    "    \n",
    "    for generation in range(generations):\n",
    "        print(f\"-\"*20)\n",
    "        print(f\"        Generation {generation + 1} (Population Size: {len(population)})\")\n",
    "\n",
    "        if len(population) == 0:\n",
    "            # No population :(, skip everything\n",
    "            print(\"No population found, breaking out!\")\n",
    "            break\n",
    "\n",
    "        # Evaluate fitness of the current population\n",
    "        fitness_scores = [calculate_fitness(member) for member in population]\n",
    "        \n",
    "        # Find the best individual in the current generation\n",
    "        # Note: If there is no best solution, it will still consider based on sorting order\n",
    "        fitness_scores = sorted(fitness_scores, key=lambda x:x['fitness_score'], reverse=True)\n",
    "        best_fitness.append(fitness_scores[0])\n",
    "\n",
    "        # Track Calculated Population\n",
    "        # We don't want to add a same parent back to population since its already been included\n",
    "        for fitness_result in fitness_scores:\n",
    "            population_processed_so_far[fitness_result['id']] = fitness_result\n",
    "\n",
    "        # Repopulate off-springs  \n",
    "        population = []      \n",
    "        for _ in range(init_population_size//2):\n",
    "            parent1, parent2 = select_parents(fitness_scores)\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            child1 = mutate(child1)\n",
    "            child2 = mutate(child2)\n",
    "\n",
    "            if scenario_unique_key(child1) not in population_processed_so_far:\n",
    "                population.append(child1)\n",
    "            \n",
    "            if scenario_unique_key(child2) not in population_processed_so_far:\n",
    "                population.append(child2)\n",
    "\n",
    "        print(f\"  Best Fitness: {fitness_scores[0]['fitness_score']}\")\n",
    "        \n",
    "    return best_fitness, population_processed_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "89735e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "        Generation 1 (Population Size: 3)\n",
      "Running command:  podman run --env-host=true -e TELEMETRY_PROMETHEUS_BACKUP=\"False\" -e WAIT_DURATION=0 -e DURATION=120  -e NAMESPACE=\"robot-shop\"  -e POD_SELECTOR=\"{service: payment}\"  -e BLOCK_TRAFFIC_TYPE=\"[Egress]\"  --net=host -v ../tmp/kubeconfig.yaml:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:application-outages\n",
      "\u001b[1m20-05-2025T15:52:56 Checking if OpenShift client is installed\u001b[0m\n",
      "/usr/bin/oc\n",
      " _              _              \n",
      "| | ___ __ __ _| | _____ _ __  \n",
      "| |/ / '__/ _` | |/ / _ \\ '_ \\ \n",
      "|   <| | | (_| |   <  __/ | | |\n",
      "|_|\\_\\_|  \\__,_|_|\\_\\___|_| |_|\n",
      "                               \n",
      "\n",
      "2025-05-20 15:52:57,442 [INFO] Starting kraken\n",
      "2025-05-20 15:52:57,449 [INFO] Initializing client to talk to the Kubernetes cluster\n",
      "2025-05-20 15:52:57,449 [INFO] Generated a uuid for the run: d67c2223-b517-4720-a7a0-6f67e6dbaabb\n",
      "2025-05-20 15:52:58,330 [INFO] Detected distribution openshift\n",
      "2025-05-20 15:53:01,993 [INFO] Fetching cluster info\n",
      "2025-05-20 15:53:04,148 [INFO] 4.18.13\n",
      "2025-05-20 15:53:04,148 [INFO] Server URL: https://api.ujoep-icmtj-7vx.c23q.p3.openshiftapps.com:443\n",
      "2025-05-20 15:53:04,149 [INFO] Daemon mode not enabled, will run through 1 iterations\n",
      "\n",
      "2025-05-20 15:53:05,436 [INFO] ðŸ“£ `ScenarioPluginFactory`: types from config.yaml mapped to respective classes for execution:\n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: application_outages_scenarios âž¡ï¸ `ApplicationOutageScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: container_scenarios âž¡ï¸ `ContainerScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: hog_scenarios âž¡ï¸ `HogsScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: managedcluster_scenarios âž¡ï¸ `ManagedClusterScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… types: [pod_disruption_scenarios, pod_network_scenarios, ingress_node_scenarios] âž¡ï¸ `NativeScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: network_chaos_scenarios âž¡ï¸ `NetworkChaosScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: network_chaos_ng_scenarios âž¡ï¸ `NetworkChaosNgScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: node_scenarios âž¡ï¸ `NodeActionsScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: pvc_scenarios âž¡ï¸ `PvcScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: service_disruption_scenarios âž¡ï¸ `ServiceDisruptionScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: service_hijacking_scenarios âž¡ï¸ `ServiceHijackingScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: cluster_shut_down_scenarios âž¡ï¸ `ShutDownScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: syn_flood_scenarios âž¡ï¸ `SynFloodScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: time_scenarios âž¡ï¸ `TimeActionsScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO]   âœ… type: zone_outages_scenarios âž¡ï¸ `ZoneOutageScenarioPlugin` \n",
      "2025-05-20 15:53:05,437 [INFO] \n",
      "\n",
      "2025-05-20 15:53:05,438 [INFO] health checks config is not defined, skipping them\n",
      "2025-05-20 15:53:05,438 [INFO] Executing scenarios for iteration 0\n",
      "2025-05-20 15:53:05,440 [INFO] Running ApplicationOutageScenarioPlugin: ['application_outages_scenarios'] -> scenarios/app_outage.yaml\n",
      "2025-05-20 15:53:05,445 [INFO] Creating the network policy\n",
      "2025-05-20 15:53:06,238 [INFO] Waiting for the specified duration in the config: 120\n",
      "2025-05-20 15:55:06,322 [INFO] Deleting the network policy\n",
      "2025-05-20 15:55:07,241 [INFO] End of scenario. Waiting for the specified duration: 0\n",
      "2025-05-20 15:55:08,136 [INFO] Find cluster events in file /tmp/events.json\n",
      "2025-05-20 15:55:08,136 [INFO] wating 0 before running the next scenario\n",
      "2025-05-20 15:55:08,136 [INFO] collecting OCP cluster metadata, this may take few minutes....\n",
      "2025-05-20 15:56:20,951 [INFO] failed to parse virtualmachines API\n",
      "2025-05-20 15:56:20,958 [INFO] Chaos data:\n",
      "{\n",
      "    \"telemetry\": {\n",
      "        \"scenarios\": [\n",
      "            {\n",
      "                \"start_timestamp\": 1747756385,\n",
      "                \"end_timestamp\": 1747756507,\n",
      "                \"scenario\": \"scenarios/app_outage.yaml\",\n",
      "                \"scenario_type\": \"application_outages_scenarios\",\n",
      "                \"exit_status\": 0,\n",
      "                \"parameters_base64\": \"\",\n",
      "                \"parameters\": {\n",
      "                    \"application_outage\": {\n",
      "                        \"block\": [\n",
      "                            \"Egress\"\n",
      "                        ],\n",
      "                        \"duration\": 120,\n",
      "                        \"namespace\": \"robot-shop\",\n",
      "                        \"pod_selector\": {\n",
      "                            \"service\": \"payment\"\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"affected_pods\": {\n",
      "                    \"recovered\": [],\n",
      "                    \"unrecovered\": [],\n",
      "                    \"error\": null\n",
      "                },\n",
      "                \"affected_nodes\": [],\n",
      "                \"cluster_events\": []\n",
      "            }\n",
      "        ],\n",
      "        \"node_summary_infos\": [\n",
      "            {\n",
      "                \"count\": 2,\n",
      "                \"architecture\": \"amd64\",\n",
      "                \"instance_type\": \"m5.xlarge\",\n",
      "                \"nodes_type\": \"worker\",\n",
      "                \"kernel_version\": \"5.14.0-427.68.1.el9_4.x86_64\",\n",
      "                \"kubelet_version\": \"v1.31.8\",\n",
      "                \"os_version\": \"Red Hat Enterprise Linux CoreOS 418.94.202505062142-0\"\n",
      "            }\n",
      "        ],\n",
      "        \"node_taints\": [],\n",
      "        \"kubernetes_objects_count\": {\n",
      "            \"ConfigMap\": 271,\n",
      "            \"Pod\": 90,\n",
      "            \"Deployment\": 35,\n",
      "            \"Route\": 9,\n",
      "            \"Build\": 1\n",
      "        },\n",
      "        \"network_plugins\": [\n",
      "            \"OVNKubernetes\"\n",
      "        ],\n",
      "        \"timestamp\": \"2025-05-20T15:53:04Z\",\n",
      "        \"health_checks\": null,\n",
      "        \"total_node_count\": 2,\n",
      "        \"cloud_infrastructure\": \"AWS\",\n",
      "        \"cloud_type\": \"rosa\",\n",
      "        \"cluster_version\": \"4.18.13\",\n",
      "        \"major_version\": \"4.18\",\n",
      "        \"run_uuid\": \"d67c2223-b517-4720-a7a0-6f67e6dbaabb\",\n",
      "        \"job_status\": true\n",
      "    },\n",
      "    \"critical_alerts\": null\n",
      "}\n",
      "2025-05-20 15:56:20,958 [INFO] telemetry collection disabled, skipping.\n",
      "2025-05-20 15:56:20,959 [INFO] Successfully finished running Kraken. UUID for the run: d67c2223-b517-4720-a7a0-6f67e6dbaabb. Report generated at kraken.report. Exiting\n",
      "Running command:  podman run --env-host=true -e TELEMETRY_PROMETHEUS_BACKUP=\"False\" -e WAIT_DURATION=0 -e DURATION=120  -e NAMESPACE=\"robot-shop\"  -e POD_SELECTOR=\"{service: rabbitmq}\"  -e BLOCK_TRAFFIC_TYPE=\"[Egress]\"  --net=host -v ../tmp/kubeconfig.yaml:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:application-outages\n",
      "\u001b[1m20-05-2025T15:56:22 Checking if OpenShift client is installed\u001b[0m\n",
      "/usr/bin/oc\n",
      " _              _              \n",
      "| | ___ __ __ _| | _____ _ __  \n",
      "| |/ / '__/ _` | |/ / _ \\ '_ \\ \n",
      "|   <| | | (_| |   <  __/ | | |\n",
      "|_|\\_\\_|  \\__,_|_|\\_\\___|_| |_|\n",
      "                               \n",
      "\n",
      "2025-05-20 15:56:22,747 [INFO] Starting kraken\n",
      "2025-05-20 15:56:22,754 [INFO] Initializing client to talk to the Kubernetes cluster\n",
      "2025-05-20 15:56:22,754 [INFO] Generated a uuid for the run: e745a212-2d36-4074-aecb-4be587e245eb\n",
      "2025-05-20 15:56:23,542 [INFO] Detected distribution openshift\n",
      "2025-05-20 15:56:26,428 [INFO] Fetching cluster info\n",
      "2025-05-20 15:56:29,163 [INFO] 4.18.13\n",
      "2025-05-20 15:56:29,164 [INFO] Server URL: https://api.ujoep-icmtj-7vx.c23q.p3.openshiftapps.com:443\n",
      "2025-05-20 15:56:29,165 [INFO] Daemon mode not enabled, will run through 1 iterations\n",
      "\n",
      "2025-05-20 15:56:30,512 [INFO] ðŸ“£ `ScenarioPluginFactory`: types from config.yaml mapped to respective classes for execution:\n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: application_outages_scenarios âž¡ï¸ `ApplicationOutageScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: container_scenarios âž¡ï¸ `ContainerScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: hog_scenarios âž¡ï¸ `HogsScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: managedcluster_scenarios âž¡ï¸ `ManagedClusterScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… types: [pod_disruption_scenarios, pod_network_scenarios, ingress_node_scenarios] âž¡ï¸ `NativeScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: network_chaos_scenarios âž¡ï¸ `NetworkChaosScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: network_chaos_ng_scenarios âž¡ï¸ `NetworkChaosNgScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: node_scenarios âž¡ï¸ `NodeActionsScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: pvc_scenarios âž¡ï¸ `PvcScenarioPlugin` \n",
      "2025-05-20 15:56:30,512 [INFO]   âœ… type: service_disruption_scenarios âž¡ï¸ `ServiceDisruptionScenarioPlugin` \n",
      "2025-05-20 15:56:30,513 [INFO]   âœ… type: service_hijacking_scenarios âž¡ï¸ `ServiceHijackingScenarioPlugin` \n",
      "2025-05-20 15:56:30,513 [INFO]   âœ… type: cluster_shut_down_scenarios âž¡ï¸ `ShutDownScenarioPlugin` \n",
      "2025-05-20 15:56:30,513 [INFO]   âœ… type: syn_flood_scenarios âž¡ï¸ `SynFloodScenarioPlugin` \n",
      "2025-05-20 15:56:30,513 [INFO]   âœ… type: time_scenarios âž¡ï¸ `TimeActionsScenarioPlugin` \n",
      "2025-05-20 15:56:30,513 [INFO]   âœ… type: zone_outages_scenarios âž¡ï¸ `ZoneOutageScenarioPlugin` \n",
      "2025-05-20 15:56:30,513 [INFO] \n",
      "\n",
      "2025-05-20 15:56:30,513 [INFO] health checks config is not defined, skipping them\n",
      "2025-05-20 15:56:30,513 [INFO] Executing scenarios for iteration 0\n",
      "2025-05-20 15:56:30,515 [INFO] Running ApplicationOutageScenarioPlugin: ['application_outages_scenarios'] -> scenarios/app_outage.yaml\n",
      "2025-05-20 15:56:30,519 [INFO] Creating the network policy\n",
      "2025-05-20 15:56:31,444 [INFO] Waiting for the specified duration in the config: 120\n",
      "2025-05-20 15:58:31,545 [INFO] Deleting the network policy\n",
      "2025-05-20 15:58:32,403 [INFO] End of scenario. Waiting for the specified duration: 0\n",
      "2025-05-20 15:58:33,142 [INFO] Find cluster events in file /tmp/events.json\n",
      "2025-05-20 15:58:33,142 [INFO] wating 0 before running the next scenario\n",
      "2025-05-20 15:58:33,142 [INFO] collecting OCP cluster metadata, this may take few minutes....\n",
      "2025-05-20 15:59:51,349 [INFO] failed to parse virtualmachines API\n",
      "2025-05-20 15:59:51,356 [INFO] Chaos data:\n",
      "{\n",
      "    \"telemetry\": {\n",
      "        \"scenarios\": [\n",
      "            {\n",
      "                \"start_timestamp\": 1747756590,\n",
      "                \"end_timestamp\": 1747756712,\n",
      "                \"scenario\": \"scenarios/app_outage.yaml\",\n",
      "                \"scenario_type\": \"application_outages_scenarios\",\n",
      "                \"exit_status\": 0,\n",
      "                \"parameters_base64\": \"\",\n",
      "                \"parameters\": {\n",
      "                    \"application_outage\": {\n",
      "                        \"block\": [\n",
      "                            \"Egress\"\n",
      "                        ],\n",
      "                        \"duration\": 120,\n",
      "                        \"namespace\": \"robot-shop\",\n",
      "                        \"pod_selector\": {\n",
      "                            \"service\": \"rabbitmq\"\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"affected_pods\": {\n",
      "                    \"recovered\": [],\n",
      "                    \"unrecovered\": [],\n",
      "                    \"error\": null\n",
      "                },\n",
      "                \"affected_nodes\": [],\n",
      "                \"cluster_events\": []\n",
      "            }\n",
      "        ],\n",
      "        \"node_summary_infos\": [\n",
      "            {\n",
      "                \"count\": 2,\n",
      "                \"architecture\": \"amd64\",\n",
      "                \"instance_type\": \"m5.xlarge\",\n",
      "                \"nodes_type\": \"worker\",\n",
      "                \"kernel_version\": \"5.14.0-427.68.1.el9_4.x86_64\",\n",
      "                \"kubelet_version\": \"v1.31.8\",\n",
      "                \"os_version\": \"Red Hat Enterprise Linux CoreOS 418.94.202505062142-0\"\n",
      "            }\n",
      "        ],\n",
      "        \"node_taints\": [],\n",
      "        \"kubernetes_objects_count\": {\n",
      "            \"ConfigMap\": 271,\n",
      "            \"Pod\": 90,\n",
      "            \"Deployment\": 35,\n",
      "            \"Route\": 9,\n",
      "            \"Build\": 1\n",
      "        },\n",
      "        \"network_plugins\": [\n",
      "            \"OVNKubernetes\"\n",
      "        ],\n",
      "        \"timestamp\": \"2025-05-20T15:56:29Z\",\n",
      "        \"health_checks\": null,\n",
      "        \"total_node_count\": 2,\n",
      "        \"cloud_infrastructure\": \"AWS\",\n",
      "        \"cloud_type\": \"rosa\",\n",
      "        \"cluster_version\": \"4.18.13\",\n",
      "        \"major_version\": \"4.18\",\n",
      "        \"run_uuid\": \"e745a212-2d36-4074-aecb-4be587e245eb\",\n",
      "        \"job_status\": true\n",
      "    },\n",
      "    \"critical_alerts\": null\n",
      "}\n",
      "2025-05-20 15:59:51,356 [INFO] telemetry collection disabled, skipping.\n",
      "2025-05-20 15:59:51,356 [INFO] Successfully finished running Kraken. UUID for the run: e745a212-2d36-4074-aecb-4be587e245eb. Report generated at kraken.report. Exiting\n",
      "Running command:  podman run --env-host=true -e TELEMETRY_PROMETHEUS_BACKUP=\"False\" -e WAIT_DURATION=0 -e DURATION=120  -e NAMESPACE=\"robot-shop\"  -e POD_SELECTOR=\"{service: shipping}\"  -e BLOCK_TRAFFIC_TYPE=\"[Ingress]\"  --net=host -v ../tmp/kubeconfig.yaml:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:application-outages\n",
      "\u001b[1m20-05-2025T15:59:52 Checking if OpenShift client is installed\u001b[0m\n",
      "/usr/bin/oc\n",
      " _              _              \n",
      "| | ___ __ __ _| | _____ _ __  \n",
      "| |/ / '__/ _` | |/ / _ \\ '_ \\ \n",
      "|   <| | | (_| |   <  __/ | | |\n",
      "|_|\\_\\_|  \\__,_|_|\\_\\___|_| |_|\n",
      "                               \n",
      "\n",
      "2025-05-20 15:59:53,148 [INFO] Starting kraken\n",
      "2025-05-20 15:59:53,154 [INFO] Initializing client to talk to the Kubernetes cluster\n",
      "2025-05-20 15:59:53,155 [INFO] Generated a uuid for the run: c477caea-4640-4503-a864-26576c6b70ba\n",
      "2025-05-20 15:59:53,880 [INFO] Detected distribution openshift\n",
      "2025-05-20 15:59:59,462 [INFO] Fetching cluster info\n",
      "2025-05-20 16:00:01,795 [INFO] 4.18.13\n",
      "2025-05-20 16:00:01,795 [INFO] Server URL: https://api.ujoep-icmtj-7vx.c23q.p3.openshiftapps.com:443\n",
      "2025-05-20 16:00:01,795 [INFO] Daemon mode not enabled, will run through 1 iterations\n",
      "\n",
      "2025-05-20 16:00:03,088 [INFO] ðŸ“£ `ScenarioPluginFactory`: types from config.yaml mapped to respective classes for execution:\n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: application_outages_scenarios âž¡ï¸ `ApplicationOutageScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: container_scenarios âž¡ï¸ `ContainerScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: hog_scenarios âž¡ï¸ `HogsScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: managedcluster_scenarios âž¡ï¸ `ManagedClusterScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… types: [pod_disruption_scenarios, pod_network_scenarios, ingress_node_scenarios] âž¡ï¸ `NativeScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: network_chaos_scenarios âž¡ï¸ `NetworkChaosScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: network_chaos_ng_scenarios âž¡ï¸ `NetworkChaosNgScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: node_scenarios âž¡ï¸ `NodeActionsScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: pvc_scenarios âž¡ï¸ `PvcScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: service_disruption_scenarios âž¡ï¸ `ServiceDisruptionScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: service_hijacking_scenarios âž¡ï¸ `ServiceHijackingScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: cluster_shut_down_scenarios âž¡ï¸ `ShutDownScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: syn_flood_scenarios âž¡ï¸ `SynFloodScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: time_scenarios âž¡ï¸ `TimeActionsScenarioPlugin` \n",
      "2025-05-20 16:00:03,088 [INFO]   âœ… type: zone_outages_scenarios âž¡ï¸ `ZoneOutageScenarioPlugin` \n",
      "2025-05-20 16:00:03,089 [INFO] \n",
      "\n",
      "2025-05-20 16:00:03,089 [INFO] health checks config is not defined, skipping them\n",
      "2025-05-20 16:00:03,089 [INFO] Executing scenarios for iteration 0\n",
      "2025-05-20 16:00:03,091 [INFO] Running ApplicationOutageScenarioPlugin: ['application_outages_scenarios'] -> scenarios/app_outage.yaml\n",
      "2025-05-20 16:00:03,095 [INFO] Creating the network policy\n",
      "2025-05-20 16:00:03,847 [INFO] Waiting for the specified duration in the config: 120\n",
      "2025-05-20 16:02:03,948 [INFO] Deleting the network policy\n",
      "2025-05-20 16:02:04,701 [INFO] End of scenario. Waiting for the specified duration: 0\n",
      "2025-05-20 16:02:05,517 [INFO] Find cluster events in file /tmp/events.json\n",
      "2025-05-20 16:02:05,517 [INFO] wating 0 before running the next scenario\n",
      "2025-05-20 16:02:05,517 [INFO] collecting OCP cluster metadata, this may take few minutes....\n",
      "2025-05-20 16:03:16,084 [INFO] failed to parse virtualmachines API\n",
      "2025-05-20 16:03:16,092 [INFO] Chaos data:\n",
      "{\n",
      "    \"telemetry\": {\n",
      "        \"scenarios\": [\n",
      "            {\n",
      "                \"start_timestamp\": 1747756803,\n",
      "                \"end_timestamp\": 1747756924,\n",
      "                \"scenario\": \"scenarios/app_outage.yaml\",\n",
      "                \"scenario_type\": \"application_outages_scenarios\",\n",
      "                \"exit_status\": 0,\n",
      "                \"parameters_base64\": \"\",\n",
      "                \"parameters\": {\n",
      "                    \"application_outage\": {\n",
      "                        \"block\": [\n",
      "                            \"Ingress\"\n",
      "                        ],\n",
      "                        \"duration\": 120,\n",
      "                        \"namespace\": \"robot-shop\",\n",
      "                        \"pod_selector\": {\n",
      "                            \"service\": \"shipping\"\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                \"affected_pods\": {\n",
      "                    \"recovered\": [],\n",
      "                    \"unrecovered\": [],\n",
      "                    \"error\": null\n",
      "                },\n",
      "                \"affected_nodes\": [],\n",
      "                \"cluster_events\": []\n",
      "            }\n",
      "        ],\n",
      "        \"node_summary_infos\": [\n",
      "            {\n",
      "                \"count\": 2,\n",
      "                \"architecture\": \"amd64\",\n",
      "                \"instance_type\": \"m5.xlarge\",\n",
      "                \"nodes_type\": \"worker\",\n",
      "                \"kernel_version\": \"5.14.0-427.68.1.el9_4.x86_64\",\n",
      "                \"kubelet_version\": \"v1.31.8\",\n",
      "                \"os_version\": \"Red Hat Enterprise Linux CoreOS 418.94.202505062142-0\"\n",
      "            }\n",
      "        ],\n",
      "        \"node_taints\": [],\n",
      "        \"kubernetes_objects_count\": {\n",
      "            \"ConfigMap\": 271,\n",
      "            \"Pod\": 90,\n",
      "            \"Deployment\": 35,\n",
      "            \"Route\": 9,\n",
      "            \"Build\": 1\n",
      "        },\n",
      "        \"network_plugins\": [\n",
      "            \"OVNKubernetes\"\n",
      "        ],\n",
      "        \"timestamp\": \"2025-05-20T16:00:01Z\",\n",
      "        \"health_checks\": null,\n",
      "        \"total_node_count\": 2,\n",
      "        \"cloud_infrastructure\": \"AWS\",\n",
      "        \"cloud_type\": \"rosa\",\n",
      "        \"cluster_version\": \"4.18.13\",\n",
      "        \"major_version\": \"4.18\",\n",
      "        \"run_uuid\": \"c477caea-4640-4503-a864-26576c6b70ba\",\n",
      "        \"job_status\": true\n",
      "    },\n",
      "    \"critical_alerts\": null\n",
      "}\n",
      "2025-05-20 16:03:16,092 [INFO] telemetry collection disabled, skipping.\n",
      "2025-05-20 16:03:16,092 [INFO] Successfully finished running Kraken. UUID for the run: c477caea-4640-4503-a864-26576c6b70ba. Report generated at kraken.report. Exiting\n",
      "  Best Fitness: 0\n",
      "--------------------\n",
      "        Generation 2 (Population Size: 2)\n",
      "Running command:  podman run --env-host=true -e TELEMETRY_PROMETHEUS_BACKUP=\"False\" -e WAIT_DURATION=0 -e DURATION=120  -e NAMESPACE=\"robot-shop\"  -e POD_SELECTOR=\"{service: shipping}\"  -e BLOCK_TRAFFIC_TYPE=\"[Egress]\"  --net=host -v ../tmp/kubeconfig.yaml:/home/krkn/.kube/config:Z containers.krkn-chaos.dev/krkn-chaos/krkn-hub:application-outages\n",
      "\u001b[1m20-05-2025T16:03:17 Checking if OpenShift client is installed\u001b[0m\n",
      "/usr/bin/oc\n",
      " _              _              \n",
      "| | ___ __ __ _| | _____ _ __  \n",
      "| |/ / '__/ _` | |/ / _ \\ '_ \\ \n",
      "|   <| | | (_| |   <  __/ | | |\n",
      "|_|\\_\\_|  \\__,_|_|\\_\\___|_| |_|\n",
      "                               \n",
      "\n",
      "2025-05-20 16:03:17,873 [INFO] Starting kraken\n",
      "2025-05-20 16:03:17,879 [INFO] Initializing client to talk to the Kubernetes cluster\n",
      "2025-05-20 16:03:17,879 [INFO] Generated a uuid for the run: fe2773db-93d5-47a0-9911-dff3a2635746\n",
      "2025-05-20 16:03:18,709 [INFO] Detected distribution openshift\n",
      "2025-05-20 16:03:26,858 [INFO] Fetching cluster info\n",
      "2025-05-20 16:03:29,128 [INFO] 4.18.13\n",
      "2025-05-20 16:03:29,128 [INFO] Server URL: https://api.ujoep-icmtj-7vx.c23q.p3.openshiftapps.com:443\n",
      "2025-05-20 16:03:29,129 [INFO] Daemon mode not enabled, will run through 1 iterations\n",
      "\n",
      "2025-05-20 16:03:30,451 [INFO] ðŸ“£ `ScenarioPluginFactory`: types from config.yaml mapped to respective classes for execution:\n",
      "2025-05-20 16:03:30,451 [INFO]   âœ… type: application_outages_scenarios âž¡ï¸ `ApplicationOutageScenarioPlugin` \n",
      "2025-05-20 16:03:30,451 [INFO]   âœ… type: container_scenarios âž¡ï¸ `ContainerScenarioPlugin` \n",
      "2025-05-20 16:03:30,451 [INFO]   âœ… type: hog_scenarios âž¡ï¸ `HogsScenarioPlugin` \n",
      "2025-05-20 16:03:30,451 [INFO]   âœ… type: managedcluster_scenarios âž¡ï¸ `ManagedClusterScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… types: [pod_disruption_scenarios, pod_network_scenarios, ingress_node_scenarios] âž¡ï¸ `NativeScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: network_chaos_scenarios âž¡ï¸ `NetworkChaosScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: network_chaos_ng_scenarios âž¡ï¸ `NetworkChaosNgScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: node_scenarios âž¡ï¸ `NodeActionsScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: pvc_scenarios âž¡ï¸ `PvcScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: service_disruption_scenarios âž¡ï¸ `ServiceDisruptionScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: service_hijacking_scenarios âž¡ï¸ `ServiceHijackingScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: cluster_shut_down_scenarios âž¡ï¸ `ShutDownScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: syn_flood_scenarios âž¡ï¸ `SynFloodScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: time_scenarios âž¡ï¸ `TimeActionsScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO]   âœ… type: zone_outages_scenarios âž¡ï¸ `ZoneOutageScenarioPlugin` \n",
      "2025-05-20 16:03:30,452 [INFO] \n",
      "\n",
      "2025-05-20 16:03:30,452 [INFO] health checks config is not defined, skipping them\n",
      "2025-05-20 16:03:30,452 [INFO] Executing scenarios for iteration 0\n",
      "2025-05-20 16:03:30,455 [INFO] Running ApplicationOutageScenarioPlugin: ['application_outages_scenarios'] -> scenarios/app_outage.yaml\n",
      "2025-05-20 16:03:30,458 [INFO] Creating the network policy\n",
      "2025-05-20 16:03:31,206 [INFO] Waiting for the specified duration in the config: 120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[251], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_fitness, all_population \u001b[38;5;241m=\u001b[39m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_population_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[248], line 21\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[0;34m(init_population_size, generations)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate fitness of the current population\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m [calculate_fitness(member) \u001b[38;5;28;01mfor\u001b[39;00m member \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Find the best individual in the current generation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Note: If there is no best solution, it will still consider based on sorting order\u001b[39;00m\n\u001b[1;32m     25\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(fitness_scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitness_score\u001b[39m\u001b[38;5;124m'\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[248], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate fitness of the current population\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmember\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m member \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Find the best individual in the current generation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Note: If there is no best solution, it will still consider based on sorting order\u001b[39;00m\n\u001b[1;32m     25\u001b[0m fitness_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(fitness_scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitness_score\u001b[39m\u001b[38;5;124m'\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[160], line 3\u001b[0m, in \u001b[0;36mcalculate_fitness\u001b[0;34m(scenario)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalculate_fitness\u001b[39m(scenario):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# run test\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     scenario_result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_krkn_scenario_with_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKUBECONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Calculate fitness\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     start \u001b[38;5;241m=\u001b[39m scenario_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[250], line 24\u001b[0m, in \u001b[0;36mrun_krkn_scenario_with_hub\u001b[0;34m(scenario, kubeconfig)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning command: \u001b[39m\u001b[38;5;124m\"\u001b[39m, krkn_hub_command)\n\u001b[1;32m     22\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 24\u001b[0m log_out \u001b[38;5;241m=\u001b[39m \u001b[43mrun_shell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkrkn_hub_command\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcmd\u001b[39m\u001b[38;5;124m\"\u001b[39m: krkn_hub_command,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: start_time,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: end_time,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\"\u001b[39m: log_out,\n\u001b[1;32m     33\u001b[0m }\n",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m, in \u001b[0;36mrun_shell\u001b[0;34m(command)\u001b[0m\n\u001b[1;32m      3\u001b[0m command \u001b[38;5;241m=\u001b[39m shlex\u001b[38;5;241m.\u001b[39msplit(command)\n\u001b[1;32m      4\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m process\u001b[38;5;241m.\u001b[39mstdout:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     logs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m line\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_fitness, all_population = genetic_algorithm(init_population_size=3, generations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185f238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc776c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
